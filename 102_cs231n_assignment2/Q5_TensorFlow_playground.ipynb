{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TensorFlow Playground**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cs231n:\n",
    "\n",
    "TensorFlow supports many other layer types, loss functions, and optimizers - you will experiment with these next. Here's the official API documentation for these (if any of the parameters used above were unclear, this resource will also be helpful). \n",
    "\n",
    "* Layers, Activations, Loss functions : https://www.tensorflow.org/api_guides/python/nn\n",
    "* Optimizers: https://www.tensorflow.org/api_guides/python/train#Optimizers\n",
    "* BatchNorm: https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = '/home/antoine/Documents/00_Datasets/CIFAR10_data'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    print(\"Subsample the data...\")\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    print(\"Normalize the data: subtract the mean image...\")\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TensorFlow Guide: Batch Normalization**\n",
    "- **ruish.io** -> [here](http://ruishu.io/2016/12/27/batchnorm/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense(x, size, scope):\n",
    "    return tf.contrib.layers.fully_connected(x, num_outputs=size, activation_fn=None, scope=scope)\n",
    "\n",
    "def dense_bn_relu(x, mode, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        h1 = tf.contrib.layers.fully_connected(x, num_outputs=100, activation_fn=None, scope='dense')\n",
    "        h2 = tf.contrib.layers.batch_norm(h1, center=True, scale=True, is_training=mode, scope='bn')\n",
    "        return tf.nn.relu(h2, name='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[None, 32, 32, 3], name='inputs')\n",
    "y = tf.placeholder(tf.int32, shape=[None], name='labels')\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "# model:\n",
    "h1 = tf.contrib.layers.flatten(X)\n",
    "h2 = dense_bn_relu(h1, mode=is_training, scope=\"layer1\")\n",
    "h3 = dense_bn_relu(h2, mode=is_training, scope=\"layer2\")\n",
    "scores = dense(h3, size=10, scope=\"scores\")\n",
    "\n",
    "with tf.name_scope(\"labels_oh\"):\n",
    "    yoh = tf.one_hot(y, depth=10)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.equal(tf.argmax(scores, 1), tf.argmax(yoh, 1))\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=yoh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'layer1/bn/cond_1/Merge:0' shape=(100,) dtype=float32>,\n",
       " <tf.Tensor 'layer1/bn/cond_1/Merge_1:0' shape=(100,) dtype=float32>,\n",
       " <tf.Tensor 'layer2/bn/cond_1/Merge:0' shape=(100,) dtype=float32>,\n",
       " <tf.Tensor 'layer2/bn/cond_1/Merge_1:0' shape=(100,) dtype=float32>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'layer1/dense/weights:0' shape=(3072, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'layer1/dense/biases:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'layer1/bn/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'layer1/bn/gamma:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'layer1/bn/moving_mean:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'layer1/bn/moving_variance:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'layer2/dense/weights:0' shape=(100, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'layer2/dense/biases:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'layer2/bn/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'layer2/bn/gamma:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'layer2/bn/moving_mean:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'layer2/bn/moving_variance:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'scores/weights:0' shape=(100, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'scores/biases:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.MODEL_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(5000):\n",
    "        batch_size = 200\n",
    "        idxs = np.random.choice(range(y_train.size), size=batch_size, replace=True)\n",
    "        Xbatch, ybatch = X_train[idxs,:,:,:], y_train[idxs]\n",
    "        \n",
    "        feed_dict = {X: Xbatch, y: ybatch, is_training: 1}\n",
    "        _, currLoss, currAcc = sess.run([train_step, loss, accuracy], feed_dict=feed_dict)\n",
    "        \n",
    "        if i%(X_train.shape[0]/batch_size)==0:\n",
    "            print(\"Epoch {:2d} : loss={:.4f} ; acc={:.2f} %\".format(i/(X_train.shape[0]/batch_size), currLoss,currAcc*100))\n",
    "            \n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-93fd337a0d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-6a7dfeff6e32>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mXbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mybatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
